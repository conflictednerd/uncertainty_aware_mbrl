_target_: src.sac.SAC


# Algorithm
cfg:
  cuda: true
  capture_video: true
  run_name: ???

  env_id: ???
  # the environment id of the task
  env_kwargs: {}
  # arguments passed to env constructor
  num_envs: 8
  # number of parallel envs 
  buffer_size: 1000000
  # the replay memory buffer size
  gamma: 0.99
  # the discount factor gamma
  tau: 0.005
  # target smoothing coefficient (default: 0.005)
  batch_size: 512
  # the batch size of sample from the reply memory
  total_timesteps: 10000000
  # total interactions with the WM for training
  learning_starts: 1e4
  # timestep to start learning
  policy_lr: 3e-4
  # the learning rate of the policy network optimizer
  q_lr: 1e-3
  # the learning rate of the Q network network optimizer
  policy_frequency: 2
  # the frequency of training policy (delayed)
  target_network_frequency: 1  # Denis Yarats' implementation delays this by 2.
  # the frequency of updates for the target nerworks
  alpha: 0.1
  # Entropy regularization coefficient.
  autotune: true
  # automatic tuning of the entropy coefficient